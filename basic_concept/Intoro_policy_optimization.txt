## Deriving the Simplest Policy Gradient
Given parameterized policy(\pi_{\theta}), the aim is maximizing the expected return J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{R(\tau)}.
For the purposes of this derivation, take R(\tau) to give the finite-horizon undiscounted return, but the derivation for the infinite-horizon discounted return setting is almost identical.

\theta_{k+1} = \theta_k + \alpha \left. \nabla_{\theta} J(\pi_{\theta}) \right|_{\theta_k}.

The gradient of policy performance, \nabla_{\theta} J(\pi_{\theta}), is called the policy gradient, and algorithms that
optimize the policy this way are called policy gradient algorithm.(Vanilla Policy Gradient & TRPO)

To actuallu use this, need an experssion for the policy gradient which can numerically compute
Step 1) Deriving the analytical gradient of policy performance, which turns out to have the form of an expected value
Step 2) Froming a sample estimate of the expected value, which can be computed with dta from a finite number of agent-environment interaction steps

## Derivation for Basic policy Gradient
See : https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html

Probability of a Trajectory + The Log-Derivative Trick + Log-Probaility of a Trajectory + 
Gradients of Environment Functions + Grad-Log-Prob of a Trajectory

## Implementing the Simplest policy gradient
1. Making the Policy Network
   - Builds modules and functions for using a feedforward neural network
   +) When we talk about a categorical distribution having 'logits', what we mean is that
      the prb. for each outcome are given by the Softmax function of the logits
2. Making the Loss function
   - Builds a "loss" function for the policy gradient algorithm
   - right data means a set of (state, action, weight) tuples collected while acting according to the current policy
   +) loss for policy gradient vs loss for supervised learning
      - The data distribution depends on the parameters
        A loss function is usually defined on a fixed data distribution which is independent of the parameters we aim to optimize
        vs
        The data must be sampled on the most recent policy
      - It doesn't measuer performance
        A loss functions is usually evaluates the performance metric
        vs
        Evaluated at the current parameters
      - But after that first step of gradient descent, there is no more connection to performance
        This mean that minimizing this losㄴ function, for a given batch of data, has no guarantee whatsoever of improving expected return
        *** if the loss goes down, all is well(normal ML). 
            In policy gradients, this intuition is wrong, and you should only care about average return. The loss function means nothing.
3. Traning 


## Expected Grad-Log-Prob Lemma
EGLP Lemma :  Suppose that P_{\theta} is a parameterized probability distribution over a random variable, x. 
Then:
      \underE{x \sim P_{\theta}}{\nabla_{\theta} \log P_{\theta}(x)} = 0.


## Reward-to-go policy gradient

\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1})}.
see : https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html

In this form, actions are only reinforced based on rewards obtained after they are taken
The sum of rewards after a point in a trajectory, 
\hat{R}_t \doteq \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}),
is called the reward-to-go from that point, and this policy gradient expression depends on the reward-to-go from state-action pairs

-->  A key problem with policy gradients is how many sample trajectories are needed to get a low-variance sample estimate for them
     The formula we started with included terms for reinforcing actions proportional to past rewards, all of which had zero mean,
     but nonzero variance: as a result, they would just add noise to sample estimates of the policy gradient. 
     By removing them, we reduce the number of sample trajectories needed.


## Baseline in Policy Gradients
An immediate consequence of the EGLP lemma is that for any function b which only depends on state,

\underE{a_t \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) b(s_t)} = 0.

This allows us to add or subtract any number of terms like this from our expression for the policy gradient, without changing it in expectation:

\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \left(\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t)\right)}.

Any function b used in this way is called a baseline.

The most common choice of baseline is the on-policy value function V^{\pi}(s_t).
Empirically, the choice b(s_t) = V^{\pi}(s_t) has the desirable effect of reducing variance in the sample estimate for the policy gradient. This results in faster and more stable policy learning

In practice, on-policy value function(V^{\pi}(s_t)) cannot be computed exactly, so it has to be approximated
This is usaully done with a neural network


## Other froms of the policy gradient
What we have seen so far is that the policy gradient has the general form

\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Phi_t},

where \Phi_t could be any of

\Phi_t &= R(\tau),

or

\Phi_t &= \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}),

or

\Phi_t &= \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t).

All of these choices lead to the same expected value for the policy gradient, despite having different variances. It turns out that there are two more valid choices of weights \Phi_t which are important to know.

1. on-policy action-value function. The choice
\Phi_t = Q^{\pi_{\theta}}(s_t, a_t)
is also valid

2.  The Advantage Function
Defined by A^{\pi}(s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t), describes how much better or worse it is than other actions on average (relative to the current policy). This choice,

\Phi_t = A^{\pi_{\theta}}(s_t, a_t)

is also valid. The proof is that it’s equivalent to using \Phi_t = Q^{\pi_{\theta}}(s_t, a_t) and then using a value function baseline, which we are always free to do.


# The formulation of policy gradients with advantage functions is extremely common, and there are many different ways of estimating the advantage function used by different algorithms.

# For a more detailed treatment of this topic, you should read the paper on Generalized Advantage Estimation (GAE), which goes into depth about different choices of \Phi_t in the background sections.

That paper then goes on to describe GAE, a method for approximating the advantage function in policy optimization algorithms which enjoys widespread use. For instance, Spinning Up’s implementations of VPG, TRPO, and PPO make use of it. As a result, we strongly advise you to study it.


